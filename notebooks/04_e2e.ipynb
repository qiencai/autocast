{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AutoCast end-to-end training and evaluation example\n",
    "\n",
    "This notebook demonstrates end-to-end training of:\n",
    "\n",
    "- autoencoder\n",
    "- flow matching\n",
    "- evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Example dataaset\n",
    "\n",
    "We use the `ReactionDiffusion` dataset as an example dataset to illustrate training and evaluation of models. This dataset simulates the advection-diffusion equation in 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.metrics import MAE, MSE, RMSE\n",
    "\n",
    "THE_WELL = False\n",
    "simulation_name = \"advection_diffusion_multichannel\"\n",
    "n_steps_input = 1\n",
    "n_steps_output = 4\n",
    "stride = 1\n",
    "rollout_stride = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Read combined data into datamodule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.data.utils import get_datamodule\n",
    "\n",
    "ae_datamodule = get_datamodule(\n",
    "    the_well=THE_WELL,\n",
    "    simulation_name=simulation_name,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    stride=stride,\n",
    "    autoencoder_mode=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "datamodule = get_datamodule(\n",
    "    the_well=THE_WELL,\n",
    "    simulation_name=simulation_name,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    stride=stride,\n",
    "    autoencoder_mode=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Set-up logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.logging import maybe_watch_model\n",
    "from autocast.logging.wandb import create_notebook_logger\n",
    "\n",
    "logger, watch = create_notebook_logger(\n",
    "    project=\"autocast-notebooks\",\n",
    "    name=f\"04_e2e_{simulation_name}\",\n",
    "    tags=[\"notebook\", simulation_name],\n",
    "    enabled=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "w, h = batch.input_fields.shape[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress by a factor of 4, with the spatial autoencoder reducing to 16x16 patches with\n",
    "# n_latent channels\n",
    "compression = 4\n",
    "latent_width, latent_height, latent_stride = 16, 16, 2\n",
    "n_latent = (w * h * n_channels // (latent_width * latent_height)) // compression\n",
    "\n",
    "print(f\"n_latent channels equals {n_latent} for a compression factor of {compression}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train autoencoder\n",
    "from autocast.decoders.dc import DCDecoder\n",
    "from autocast.encoders.dc import DCEncoder\n",
    "from autocast.models.autoencoder import AE\n",
    "\n",
    "encoder = DCEncoder(\n",
    "    in_channels=n_channels,\n",
    "    out_channels=n_latent,\n",
    "    # hid_channels=[32, 64],\n",
    "    # hid_blocks=[2, 2],\n",
    "    # e.g. for extra factor of 2 downsampling\n",
    "    hid_channels=[32, 64, 128],\n",
    "    hid_blocks=[2, 2, 2],\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    spatial=2,\n",
    "    pixel_shuffle=False,\n",
    "    periodic=False,\n",
    "    dropout=None,\n",
    ")\n",
    "decoder = DCDecoder(\n",
    "    in_channels=n_latent,\n",
    "    out_channels=n_channels,\n",
    "    # hid_channels=[64, 32],\n",
    "    # hid_blocks=[2, 2],\n",
    "    # e.g. for extra factor of 2 downsampling\n",
    "    hid_channels=[128, 64, 32],\n",
    "    hid_blocks=[2, 2, 2],\n",
    "    kernel_size=3,\n",
    "    stride=2,\n",
    "    spatial=2,\n",
    "    pixel_shuffle=False,\n",
    "    periodic=False,\n",
    "    dropout=None,\n",
    ")\n",
    "\n",
    "ae = AE(encoder=encoder, decoder=decoder, learning_rate=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded, global_cond = ae.encoder.encode_with_cond(next(iter(ae_datamodule.train_dataloader())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encoded shape is:\", tuple(encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "device = \"mps\"  # \"cpu\"\n",
    "trainer = L.Trainer(max_epochs=2, accelerator=device, logger=logger)\n",
    "trainer.fit(ae, ae_datamodule.train_dataloader(), ae_datamodule.val_dataloader())\n",
    "trainer.save_checkpoint(f\"./{simulation_name}_ae_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cpu\"\n",
    "num_examples = 2\n",
    "for idx, batch in enumerate(ae_datamodule.test_dataloader()):\n",
    "    inputs = batch.input_fields.to(device)\n",
    "    outputs, latents = ae.forward_with_latent(batch)\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "    print(\"Latent shape:\", latents.shape)\n",
    "    input_frame = inputs[0, 0, :, :, 1].detach().cpu().numpy()\n",
    "    output_frame = outputs[0, 0, :, :, 1].detach().cpu().numpy()\n",
    "    latent_frame = latents[0, 0, :, :, 0].detach().cpu().numpy()\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(10, 4))\n",
    "    axs[0].imshow(input_frame, cmap=\"viridis\")\n",
    "    axs[0].set_title(\"Input\")\n",
    "    axs[1].imshow(output_frame, cmap=\"viridis\")\n",
    "    axs[1].set_title(\"Reconstruction\")\n",
    "    axs[2].imshow(output_frame - input_frame, cmap=\"viridis\")\n",
    "    axs[2].set_title(\"Difference\")\n",
    "    axs[3].imshow(latent_frame, cmap=\"viridis\")\n",
    "    axs[3].set_title(\"Latent\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if idx + 1 >= num_examples:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Example shape and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.train_dataset[0].input_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "batch.input_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azula.noise import VPSchedule\n",
    "\n",
    "from autocast.metrics.deterministic import VRMSE\n",
    "from autocast.models.encoder_processor_decoder import EncoderProcessorDecoder\n",
    "from autocast.nn.base import TemporalBackboneBase\n",
    "from autocast.nn.unet import TemporalUNetBackbone\n",
    "from autocast.nn.vit import TemporalViTBackbone\n",
    "from autocast.processors.diffusion import DiffusionProcessor\n",
    "from autocast.processors.flow_matching import FlowMatchingProcessor\n",
    "\n",
    "# Get sample batch\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "example_global_cond = encoder.encode_cond(batch)\n",
    "global_cond_channels = (\n",
    "    example_global_cond.shape[-1]\n",
    "    if example_global_cond is not None\n",
    "    else None\n",
    ")\n",
    "\n",
    "# Construct backbone\n",
    "\n",
    "def get_backbone(backbone_name: str) -> TemporalBackboneBase:\n",
    "    \"\"\"Create backbone based on name.\"\"\"\n",
    "    if backbone_name == \"unet\":\n",
    "        backbone = TemporalUNetBackbone(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            cond_channels=n_channels,\n",
    "            n_steps_output=n_steps_output,\n",
    "            n_steps_input=n_steps_input,\n",
    "            global_cond_channels=global_cond_channels,\n",
    "            mod_features=200,\n",
    "            hid_channels=(32, 64, 128),\n",
    "            hid_blocks=(2, 2, 2),\n",
    "            spatial=2,\n",
    "            periodic=False,\n",
    "        )\n",
    "    elif backbone_name == \"vit\":\n",
    "        backbone = TemporalViTBackbone(\n",
    "            in_channels=n_channels,\n",
    "            out_channels=n_channels,\n",
    "            cond_channels=n_channels,\n",
    "            n_steps_output=n_steps_output,\n",
    "            n_steps_input=n_steps_input,\n",
    "            mod_features=256,\n",
    "            global_cond_channels=global_cond_channels,\n",
    "            # hid_channels=768,\n",
    "            hid_channels=512,\n",
    "            # hid_blocks=12,\n",
    "            hid_blocks=8,\n",
    "            temporal_method=\"none\",\n",
    "            # temporal_method=\"attention\",\n",
    "            # attention_heads=12,\n",
    "            attention_heads=8,\n",
    "            spatial=2,\n",
    "            patch_size=1,\n",
    "            dropout=0.05,\n",
    "            ffn_factor=4,\n",
    "            checkpointing=True,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone name: {backbone_name}\")\n",
    "    return backbone\n",
    "\n",
    "\n",
    "backbone_name = \"vit\"  # set to \"unet\" or \"vit\"\n",
    "backbone = get_backbone(backbone_name)\n",
    "\n",
    "\n",
    "# Construct processor\n",
    "\n",
    "processor_name = \"flow_matching\"  # set to \"diffusion\" to compare\n",
    "# processor_name = \"diffusion\"  # set to \"flow_matching\" to compare\n",
    "\n",
    "if processor_name == \"flow_matching\":\n",
    "    processor = FlowMatchingProcessor(\n",
    "        backbone=backbone,\n",
    "        n_steps_output=n_steps_output,\n",
    "        n_channels_out=n_latent,\n",
    "        flow_ode_steps=20,\n",
    "    )\n",
    "else:\n",
    "    processor = DiffusionProcessor(\n",
    "        backbone=backbone,\n",
    "        schedule=VPSchedule(),\n",
    "        n_steps_output=n_steps_output,\n",
    "        n_channels_out=n_latent,\n",
    "    )\n",
    "\n",
    "model = EncoderProcessorDecoder(\n",
    "    encoder_decoder=ae,\n",
    "    processor=processor,\n",
    "    train_in_latent_space=True,\n",
    "    learning_rate=5e-4,\n",
    "    val_metrics=[VRMSE(), MSE(), MAE(), RMSE()],\n",
    "    test_metrics=[VRMSE(), MSE(), MAE(), RMSE()],\n",
    ")\n",
    "maybe_watch_model(logger, model, watch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Run trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "device = \"mps\"  # \"cpu\"\n",
    "trainer = L.Trainer(max_epochs=5, accelerator=device, logger=logger)\n",
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "trainer.save_checkpoint(f\"./{simulation_name}_{processor_name}_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Run the evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Example rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single element is the full trajectory\n",
    "datamodule = get_datamodule(\n",
    "    the_well=THE_WELL,\n",
    "    simulation_name=simulation_name,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    stride=stride,\n",
    "    autoencoder_mode=False,\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "batch = next(iter(datamodule.rollout_test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n_steps_input are inputs\n",
    "print(batch.input_fields.shape)\n",
    "# Remaining n_steps_output are outputs\n",
    "print(batch.output_fields.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.models.encoder_processor_decoder_ensemble import (\n",
    "    EncoderProcessorDecoderEnsemble,\n",
    ")\n",
    "\n",
    "ensemble_model = EncoderProcessorDecoderEnsemble(\n",
    "    encoder_decoder=model.encoder_decoder,\n",
    "    processor=model.processor,\n",
    "    train_in_latent_space=False,\n",
    "    learning_rate=5e-4,\n",
    "    test_metrics = [],\n",
    "    val_metrics = [],\n",
    "    n_members=5,\n",
    "    batch_size=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rollout on one trajectory\n",
    "preds, trues = ensemble_model.rollout(\n",
    "    batch, stride=rollout_stride, max_rollout_steps=80, free_running_only=True,\n",
    "    n_members=5,\n",
    ")\n",
    "\n",
    "print(preds.shape) # B, T, H, W, C, M\n",
    "assert trues is not None\n",
    "print(trues.shape) # B, T, H, W, C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.metrics import MSE\n",
    "\n",
    "assert trues is not None\n",
    "assert preds.shape == trues.shape\n",
    "mse = MSE()\n",
    "mse_error = mse(preds, trues)\n",
    "print(\"MSE overall is a single scalar:\", mse_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from autocast.utils import plot_spatiotemporal_video\n",
    "\n",
    "batch_idx = 0\n",
    "if simulation_name == \"advection_diffusion_multichannel\":\n",
    "    channel_names = [\"vorticity\", \"velocity_x\", \"velocity_y\", \"streamfunction\"]\n",
    "elif simulation_name == \"advection_diffusion\":\n",
    "    channel_names = [\"vorticity\"]\n",
    "elif simulation_name == \"reaction_diffusion\":\n",
    "    channel_names = [\"U\", \"V\"]\n",
    "else:\n",
    "    channel_names = None\n",
    "\n",
    "anim = plot_spatiotemporal_video(\n",
    "    pred=preds.mean(dim=-1),\n",
    "    true=trues[..., 0],\n",
    "    pred_uq=preds.std(dim=-1),\n",
    "    batch_idx=batch_idx,\n",
    "    save_path=f\"{simulation_name}_{batch_idx:02d}.mp4\",\n",
    "    colorbar_mode=\"column\",\n",
    "    channel_names=channel_names,\n",
    "    pred_uq_label=\"Ensemble Std. Dev.\",\n",
    "    colorbar_mode_uq=\"row\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot in notebook\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
