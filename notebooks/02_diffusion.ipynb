{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# BOUT - Diffusion test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from auto_cast.data.dataset import BOUTDataset\n",
    "from auto_cast.data.datamodule import SpatioTemporalDataModule\n",
    "from azula.noise import CosineSchedule\n",
    "from auto_cast.types import EncodedBatch\n",
    "data_path=\"data/bout_split\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 2. Load Data\n",
    "# ============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_steps_input = 1\n",
    "n_steps_output = 4\n",
    "datamodule = SpatioTemporalDataModule(\n",
    "    data_path=data_path,\n",
    "    dataset_cls=BOUTDataset,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    stride=1,\n",
    "    batch_size=1,\n",
    "    dtype=torch.float32,\n",
    "    ftype=\"torch\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "train_loader = datamodule.train_dataloader()\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "batch.input_fields.shape, batch.output_fields.shape, batch.constant_scalars.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "from auto_cast.encoders.base import Encoder\n",
    "from auto_cast.types import Batch, Tensor, TensorBCWH\n",
    "\n",
    "\n",
    "class IdentityEncoder(Encoder):\n",
    "    \"\"\"Permute and concatenate Encoder.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, batch: Batch) -> Tensor:\n",
    "        return batch.input_fields\n",
    "\n",
    "    def encode(self, batch: Batch) -> TensorBCWH:\n",
    "        return self.forward(batch)\n",
    "    \n",
    "from einops import rearrange\n",
    "\n",
    "from auto_cast.decoders.base import Decoder\n",
    "from auto_cast.types import TensorBCTSPlus, TensorBMStarL, TensorBTSPlusC\n",
    "\n",
    "\n",
    "class IdentityDecoder(Decoder):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    def forward(self, x: TensorBCTSPlus) -> TensorBTSPlusC:\n",
    "        return x\n",
    "\n",
    "    def decode(self, z: TensorBTSPlusC) -> TensorBTSPlusC:\n",
    "        return self.forward(z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Wrap Azula UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import cond\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from azula.nn.unet import UNet\n",
    "from azula.nn.embedding import SineEncoding\n",
    "\n",
    "class TemporalUNetBackbone(nn.Module):\n",
    "    \"\"\"Azula UNet with proper time embedding.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 1,\n",
    "        cond_channels: int = 1,\n",
    "        mod_features: int = 256,\n",
    "        hid_channels: tuple = (32, 64, 128),\n",
    "        hid_blocks: tuple = (2, 2, 2),\n",
    "        spatial: int = 2,\n",
    "        periodic: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_embedding = nn.Sequential(\n",
    "            SineEncoding(mod_features),\n",
    "            nn.Linear(mod_features, mod_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(mod_features, mod_features),\n",
    "        )\n",
    "        \n",
    "        self.unet = UNet(\n",
    "            in_channels=in_channels + cond_channels,\n",
    "            out_channels=out_channels,\n",
    "            cond_channels=0,\n",
    "            mod_features=mod_features,\n",
    "            hid_channels=hid_channels,\n",
    "            hid_blocks=hid_blocks,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            spatial=spatial,\n",
    "            periodic=periodic,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_out: torch.Tensor, t: torch.Tensor, cond: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_out: Noisy data (B, T, C, H, W) - channels first from Azula\n",
    "            t: Time steps (B,)\n",
    "            cond: Conditioning input (B, T_cond, C, H, W) - channels first\n",
    "        Returns:\n",
    "            Denoised output (B, T, C, H, W)\n",
    "        \"\"\"\n",
    "        B, T, W, H, C = x_out.shape\n",
    "        _, T_cond, W_cond, H_cond , C_cond = cond.shape\n",
    "        assert W == W_cond and H == H_cond\n",
    "\n",
    "        # Embed time (once per batch)\n",
    "        t_emb = self.time_embedding(t)  # (B, mod_features)\n",
    "        mod_for_unet = t_emb\n",
    "        t_emb = rearrange(t_emb, \"b m -> b  1 1 1 m\")\n",
    "        t_emb = t_emb.expand(B, T_cond, W, H, -1)  # (B, mod_features, H, W)\n",
    "\n",
    "        # Concatenate along channel dimension\n",
    "        x_cond = torch.cat([cond, t_emb], dim=-1)  # (B, T, C+C_cond, H, W)\n",
    "        \n",
    "        x_cond = rearrange(x_cond, \"b t w h c -> b (t c) w h\")\n",
    "        # Process through UNet\n",
    "        out_flat = self.unet(x_cond, mod=mod_for_unet)\n",
    "        # Reshape back to (B, T, C, H, W)\n",
    "        return out_flat.reshape(B, T, W, H, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 3. Create DiffusionProcessor\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.output_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_cast.decoders.channels_last import ChannelsLast\n",
    "from auto_cast.encoders.permute_concat import PermuteConcat\n",
    "from auto_cast.models.encoder_decoder import EncoderDecoder\n",
    "from auto_cast.models.encoder_processor_decoder import EncoderProcessorDecoder\n",
    "from auto_cast.processors.diffusion import DiffusionProcessor\n",
    "from azula.noise import CosineSchedule\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "# Create schedule\n",
    "schedule = CosineSchedule()\n",
    "mod_features = 64\n",
    "backbone = TemporalUNetBackbone(\n",
    "    in_channels=(n_channels+mod_features)*n_steps_input,          # 1\n",
    "    out_channels=n_channels* n_steps_output,         # 1\n",
    "    cond_channels=0,        # 1\n",
    "    mod_features=mod_features,\n",
    "    hid_channels=(16, 32, 64),\n",
    "    hid_blocks=(2, 2, 2),\n",
    "    spatial=2,\n",
    "    periodic=False,\n",
    ")\n",
    "\n",
    "\n",
    "processor = DiffusionProcessor(\n",
    "    backbone=backbone,\n",
    "    schedule=schedule,\n",
    "    denoiser_type='karras',\n",
    "    learning_rate=1e-4,\n",
    "    n_steps_output=n_steps_output,  # 4\n",
    "    stride=1,\n",
    "    max_rollout_steps=10,\n",
    "    teacher_forcing_ratio=0.0,\n",
    ")\n",
    "encoder = IdentityEncoder()\n",
    "decoder = IdentityDecoder()\n",
    "\n",
    "model = EncoderProcessorDecoder.from_encoder_processor_decoder(\n",
    "    encoder_decoder=EncoderDecoder.from_encoder_decoder(\n",
    "        encoder=encoder, decoder=decoder\n",
    "    ),\n",
    "    processor=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# 4. Test Forward Pass\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "# device = \"cpu\"\n",
    "trainer = L.Trainer(max_epochs=5, accelerator=device, log_every_n_steps=10, precision=\"16-mixed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load WITH the components\n",
    "model = EncoderProcessorDecoder.load_from_checkpoint(\n",
    "    \"lightning_logs/version_3/checkpoints/epoch=19-step=18340.ckpt\",\n",
    "    encoder_decoder=EncoderDecoder.from_encoder_decoder(\n",
    "        encoder=encoder, decoder=decoder\n",
    "    ),\n",
    "    processor=processor,\n",
    "    strict=False\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.rollout_test_dataloader()))\n",
    "# First n_steps_input are inputs\n",
    "print(batch.input_fields.shape)\n",
    "# Remaining n_steps_output are outputs\n",
    "print(batch.output_fields.shape)\n",
    "preds, trues = model.rollout(batch, free_running_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Create Side-by-Side Animated GIF from Rollout\n",
    "# ============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import PillowWriter\n",
    "\n",
    "# preds and trues are already available from your rollout!\n",
    "print(f\"Predictions shape: {preds.shape}\")  # [B, R, T, H, W, C]\n",
    "print(f\"Ground truth shape: {trues.shape if trues is not None else 'None'}\")\n",
    "\n",
    "# Select sample to visualize\n",
    "sample_idx = 0\n",
    "\n",
    "# Extract predictions and ground truth\n",
    "pred_rollout = preds[sample_idx].cpu().detach().numpy()  # [R, T, H, W, C]\n",
    "if trues is not None:\n",
    "    true_rollout = trues[sample_idx].cpu().detach().numpy()  # [R, T, H, W, C]\n",
    "else:\n",
    "    true_rollout = np.zeros_like(pred_rollout)\n",
    "\n",
    "# Get dimensions\n",
    "num_rollout_windows, timesteps_per_window, H, W, C = pred_rollout.shape\n",
    "print(f\"\\nRollout windows: {num_rollout_windows}\")\n",
    "print(f\"Timesteps per window: {timesteps_per_window}\")\n",
    "print(f\"Spatial size: {H}x{W}\")\n",
    "print(f\"Channels: {C}\")\n",
    "\n",
    "# Create sequence: take first timestep from each rollout window\n",
    "pred_sequence = pred_rollout[:, 0, ...]  # [R, H, W, C]\n",
    "true_sequence = true_rollout[:, 0, ...]  # [R, H, W, C]\n",
    "\n",
    "# Prepend initial condition\n",
    "initial_frame = batch.input_fields[sample_idx, -1].cpu().numpy()  # [H, W, C]\n",
    "pred_sequence = np.concatenate([initial_frame[None, ...], pred_sequence], axis=0)\n",
    "true_sequence = np.concatenate([initial_frame[None, ...], true_sequence], axis=0)\n",
    "\n",
    "# Squeeze channel dimension if C=1\n",
    "if C == 1:\n",
    "    pred_sequence = pred_sequence.squeeze(-1)  # [Frames, H, W]\n",
    "    true_sequence = true_sequence.squeeze(-1)\n",
    "\n",
    "num_frames = pred_sequence.shape[0]\n",
    "print(f\"Total frames in sequence: {num_frames}\")\n",
    "\n",
    "# Normalize for visualization\n",
    "vmin = min(pred_sequence.min(), true_sequence.min())\n",
    "vmax = max(pred_sequence.max(), true_sequence.max())\n",
    "print(f\"Data range: [{vmin:.4f}, {vmax:.4f}]\")\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - vmin) / (vmax - vmin) if vmax > vmin else x\n",
    "\n",
    "# Create figure with side-by-side plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Initialize images\n",
    "im_pred = axes[0].imshow(normalize(pred_sequence[0]), cmap='viridis', vmin=0, vmax=1)\n",
    "axes[0].set_title(f'Prediction (Frame 0/{num_frames-1})')\n",
    "axes[0].axis('off')\n",
    "plt.colorbar(im_pred, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im_true = axes[1].imshow(normalize(true_sequence[0]), cmap='viridis', vmin=0, vmax=1)\n",
    "axes[1].set_title(f'Ground Truth (Frame 0/{num_frames-1})')\n",
    "axes[1].axis('off')\n",
    "plt.colorbar(im_true, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Animation update function\n",
    "def update(frame_idx):\n",
    "    im_pred.set_array(normalize(pred_sequence[frame_idx]))\n",
    "    axes[0].set_title(f'Prediction (Frame {frame_idx}/{num_frames-1})')\n",
    "    \n",
    "    im_true.set_array(normalize(true_sequence[frame_idx]))\n",
    "    axes[1].set_title(f'Ground Truth (Frame {frame_idx}/{num_frames-1})')\n",
    "    \n",
    "    return [im_pred, im_true]\n",
    "\n",
    "# Create animation\n",
    "print(\"\\nCreating animation...\")\n",
    "anim = animation.FuncAnimation(\n",
    "    fig,\n",
    "    update,\n",
    "    frames=num_frames,\n",
    "    interval=100,  # 100ms per frame = 10 fps\n",
    "    blit=True,\n",
    "    repeat=True\n",
    ")\n",
    "\n",
    "# Save as GIF\n",
    "print(\"Saving GIF...\")\n",
    "writer = PillowWriter(fps=10)\n",
    "anim.save('rollout_comparison.gif', writer=writer)\n",
    "print(\"âœ“ Saved to: rollout_comparison.gif\")\n",
    "\n",
    "# Display in notebook\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename='rollout_comparison.gif'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-cast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
