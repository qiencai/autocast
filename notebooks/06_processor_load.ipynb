{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AutoCast Processor Evaluation\n",
    "\n",
    "This notebook evaluates a pre-trained processor model on the MiniWell dataset.\n",
    "It loads the model configuration and weights from a specified run directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from IPython.display import HTML\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from autocast.external.lola.lola_autoencoder import get_autoencoder\n",
    "from autocast.models.processor import ProcessorModel\n",
    "from autocast.utils.plots import plot_spatiotemporal_video\n",
    "\n",
    "device = \"mps\"  # or \"cpu\"\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the run directory\n",
    "run_path = \"../outputs/rayleigh_benard/2026-01-14_diffusion_vit_small\"\n",
    "config_path = os.path.join(run_path, \"resolved_processor_config.yaml\")\n",
    "ckpt_path = os.path.join(run_path, \"processor.ckpt\")\n",
    "\n",
    "# Load configuration\n",
    "cfg = OmegaConf.load(config_path)\n",
    "# print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataModule and setup\n",
    "datamodule = instantiate(cfg.data)\n",
    "datamodule.setup()  # Setup all stages (fit for train/val, test for test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Processor\n",
    "processor = instantiate(cfg.model.processor)\n",
    "\n",
    "# Instantiate ProcessorModelWrapper\n",
    "model = ProcessorModel(processor=processor, learning_rate=cfg.model.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(ckpt_path, weights_only=True, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do rollout prediction first\n",
    "from autocast.types.batch import EncodedBatch\n",
    "\n",
    "batch = next(iter(datamodule.val_dataloader()))\n",
    "\n",
    "# Reduce batch size for faster inference\n",
    "batch = EncodedBatch(\n",
    "    encoded_inputs=batch.encoded_inputs[:1],\n",
    "    encoded_output_fields=batch.encoded_output_fields[:1],\n",
    "    global_cond=batch.global_cond[:1] if batch.global_cond is not None else None,\n",
    "    encoded_info={},\n",
    ")\n",
    "with torch.no_grad():\n",
    "    preds_free_running = model.rollout(\n",
    "        batch,\n",
    "        stride=batch.encoded_output_fields.shape[1],\n",
    "        max_rollout_steps=20,\n",
    "        free_running_only=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rollout with teacher forcing\n",
    "with torch.no_grad():\n",
    "    preds, trues = [], []\n",
    "    for i, batch in enumerate(datamodule.val_dataloader()):\n",
    "        pred = model(batch.encoded_inputs, batch.global_cond)\n",
    "        preds.append(pred)\n",
    "        trues.append(batch.encoded_output_fields)\n",
    "        if i >= 3:  # Limit for demonstration purposes\n",
    "            break\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    trues = torch.cat(trues, dim=0)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Ground Truth shape: {trues.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "max_rollout_steps = 15\n",
    "dataset_stride: int = datamodule.stride\n",
    "indices = torch.arange(0, max_rollout_steps) * dataset_stride\n",
    "indices = indices[indices < trues.shape[0]]\n",
    "# trues_rollout = rearrange(trues[indices, 0], \"B ... C -> 1 B ... C\")\n",
    "# preds_rollout = rearrange(preds[indices, 0], \"B ... C -> 1 B ... C\")\n",
    "trues_rollout = rearrange(trues[indices[::4]], \"B T ... C -> 1 (B T) ... C\")\n",
    "preds_rollout = rearrange(preds[indices[::4]], \"B T ... C -> 1 (B T) ... C\")\n",
    "print(f\"Constructed Ground Truth shape: {trues_rollout.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = plot_spatiotemporal_video(\n",
    "    true=trues_rollout[..., :4] if trues_rollout is not None else None,\n",
    "    pred=preds_rollout[..., :4],\n",
    "    batch_idx=0,\n",
    "    save_path=\"teacher_forcing_prediction.mp4\",\n",
    "    title=\"Teacher Forcing (latent)\",\n",
    "    colorbar_mode=\"row\",\n",
    ")\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AutoEncoder to decode predictions\n",
    "ae_path = \"../datasets/rayleigh_benard/1e3z5x2c_rayleigh_benard_dcae_f32c64_large\"\n",
    "ae_config_path = os.path.join(ae_path, \"config.yaml\")\n",
    "ae_ckpt_path = os.path.join(ae_path, \"state.pth\")\n",
    "\n",
    "print(f\"Loading AutoEncoder from: {ae_path}\")\n",
    "ae_cfg = OmegaConf.load(ae_config_path)\n",
    "\n",
    "# Convert to dictionary to avoid OmegaConf/beartype conflicts for most args (like attention_heads)\n",
    "ae_config_dict = OmegaConf.to_container(ae_cfg.ae, resolve=True)\n",
    "\n",
    "# However, get_autoencoder specifically types 'loss' as DictConfig, so we must preserve it\n",
    "if \"loss\" in ae_cfg.ae:\n",
    "    ae_config_dict[\"loss\"] = ae_cfg.ae.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.external.lola.wrapped_decoder import WrappedDecoder\n",
    "from autocast.external.lola.wrapped_encoder import WrappedEncoder\n",
    "from autocast.models.autoencoder import AE\n",
    "\n",
    "encoder = WrappedEncoder(**ae_config_dict)  # type: ignore  # noqa: PGH003\n",
    "decoder = WrappedDecoder(device=device, batch_size=4, runpath=ae_path, **ae_config_dict)  # type: ignore  # noqa: PGH003\n",
    "ae = AE(encoder=encoder, decoder=decoder)\n",
    "_ = ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode\n",
    "with torch.no_grad():\n",
    "    trues_decoded = ae.decode(trues_rollout)\n",
    "    preds_decoded = ae.decode(preds_rollout)\n",
    "    preds_free_running_decoded = ae.decode(preds_free_running[0])\n",
    "\n",
    "    # Swap to (B, T, H, W, C) and flip for plotting\n",
    "    trues_decoded = rearrange(trues_decoded, \"1 T H W C -> 1 T W H C\").flip(-3)\n",
    "    preds_decoded = rearrange(preds_decoded, \"1 T H W C -> 1 T W H C\").flip(-3)\n",
    "    preds_free_running_decoded = rearrange(\n",
    "        preds_free_running_decoded, \"1 T H W C -> 1 T W H C\"\n",
    "    ).flip(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decoded\n",
    "anim = plot_spatiotemporal_video(\n",
    "    true=trues_decoded,\n",
    "    pred=preds_decoded,\n",
    "    batch_idx=0,\n",
    "    save_path=\"teacher_forcing_decoded_prediction.mp4\",\n",
    "    title=\"Teacher forcing (decoded) prediction\",\n",
    "    colorbar_mode=\"row\",\n",
    ")\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot decoded\n",
    "anim = plot_spatiotemporal_video(\n",
    "    true=trues_decoded,\n",
    "    pred=preds_free_running_decoded[:, : trues_decoded.shape[1]],\n",
    "    batch_idx=0,\n",
    "    save_path=\"free_running_decoded_prediction.mp4\",\n",
    "    title=\"Free running (decoded) prediction\",\n",
    "    colorbar_mode=\"row\",\n",
    ")\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
