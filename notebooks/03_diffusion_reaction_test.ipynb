{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoemulate.simulations.advection_diffusion import AdvectionDiffusion as Sim\n",
    "\n",
    "sim = Sim(return_timeseries=True, log_level=\"error\")\n",
    "\n",
    "\n",
    "def generate_split(simulator: Sim, n_train: int = 10, n_valid: int = 2, n_test: int = 2):\n",
    "    \"\"\"Generate training, validation, and test splits from the simulator.\"\"\"\n",
    "    train = simulator.forward_samples_spatiotemporal(n_train)\n",
    "    valid = simulator.forward_samples_spatiotemporal(n_valid)\n",
    "    test = simulator.forward_samples_spatiotemporal(n_test)\n",
    "    return {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "\n",
    "\n",
    "combined_data = generate_split(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.data.datamodule import SpatioTemporalDataModule\n",
    "\n",
    "n_steps_input = 1\n",
    "n_steps_output = 4\n",
    "datamodule = SpatioTemporalDataModule(\n",
    "    data=combined_data,\n",
    "    data_path=None,\n",
    "    n_steps_input=n_steps_input,\n",
    "    n_steps_output=n_steps_output,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "batch.input_fields.shape, batch.output_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from azula.noise import CosineSchedule\n",
    "\n",
    "from autocast.decoders.identity import IdentityDecoder\n",
    "from autocast.encoders.identity import IdentityEncoder\n",
    "from autocast.models.encoder_decoder import EncoderDecoder\n",
    "from autocast.models.encoder_processor_decoder import EPDTrainProcessor\n",
    "from autocast.nn.unet import TemporalUNetBackbone\n",
    "from autocast.nn.vit import TemporalViTBackbone\n",
    "from autocast.processors.diffusion import DiffusionProcessor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Setup the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "print(\"Number of channels:\", n_channels)\n",
    "# Create schedule\n",
    "schedule = CosineSchedule()\n",
    "mod_features = 128\n",
    "\n",
    "backbone = TemporalViTBackbone(\n",
    "    in_channels=n_channels,\n",
    "    out_channels=n_channels,\n",
    "    cond_channels=n_channels,\n",
    "    mod_features=mod_features,\n",
    "    n_steps_output=n_steps_output,\n",
    "    n_steps_input=n_steps_input,\n",
    "    hid_channels=512,                # ViT hidden dimension\n",
    "    hid_blocks=8,                    # Number of transformer blocks\n",
    "    attention_heads=8,                     # ViT attention heads\n",
    "    patch_size=5,                    # Spatial patch size\n",
    "    spatial=2,\n",
    "    temporal_method=\"attention\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Initiate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 320\n",
    "\n",
    "# Calculate max rollout steps needed\n",
    "max_rollout_steps = total_timesteps - n_steps_input  # 320 - 1 = 319\n",
    "stride =4\n",
    "# Update your processor\n",
    "processor = DiffusionProcessor(\n",
    "    backbone=backbone,\n",
    "    schedule=schedule,\n",
    "    denoiser_type='karras',\n",
    "    learning_rate=1e-4,\n",
    "    n_steps_output=n_steps_output,  # Still 4 (window size)\n",
    ")\n",
    "encoder = IdentityEncoder()\n",
    "decoder = IdentityDecoder()\n",
    "\n",
    "\n",
    "model = EPDTrainProcessor(\n",
    "    encoder_decoder=EncoderDecoder(\n",
    "        encoder=encoder, decoder=decoder\n",
    "    ),\n",
    "    processor=processor,\n",
    "    max_rollout_steps=max_rollout_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from autocast.logging import create_wandb_logger, maybe_watch_model\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    "    )\n",
    "print(\"Using device:\", device)\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.logging.wandb import create_notebook_logger\n",
    "\n",
    "wandb_logger, wandb_watch = create_notebook_logger(\n",
    "    project=\"autocast-notebooks\",\n",
    "    name=\"03_diffusion_reaction_test\",\n",
    "    tags=[\"notebook\", \"03-diffusion-reaction-test\"]\n",
    ")\n",
    "trainer = L.Trainer(max_epochs=3, accelerator=device, log_every_n_steps=10, logger=wandb_logger)\n",
    "maybe_watch_model(wandb_logger, model, wandb_watch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.stride = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, do full rollout\n",
    "model.max_rollout_steps = 5\n",
    "model.processor.max_rollout_steps = 5\n",
    "stride = 4\n",
    "batch = next(iter(datamodule.rollout_test_dataloader()))\n",
    "preds, trues = model.rollout(batch, stride=stride, free_running_only=True)\n",
    "\n",
    "print(f\"Predictions shape: {preds.shape}\")  # Should be [B, 319, 4, 50, 50, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.mean(), trues.mean() # type: ignore\n",
    "preds.std(), trues.std() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from autocast.utils import plot_spatiotemporal_video\n",
    "\n",
    "anim = plot_spatiotemporal_video(\n",
    "    pred=preds,\n",
    "    true=trues,\n",
    "    cmap=\"plasma\",\n",
    ")\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-cast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
