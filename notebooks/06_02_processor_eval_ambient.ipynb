{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AutoCast Processor Evaluation\n",
    "\n",
    "This notebook evaluates a pre-trained processor model on the MiniWell dataset.\n",
    "It loads the model configuration and weights from a specified run directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from IPython.display import HTML\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from autocast.models.processor import ProcessorModel\n",
    "from autocast.utils.plots import plot_spatiotemporal_video\n",
    "\n",
    "# device = \"mps\"  # or \"cpu\"\n",
    "# device = \"cpu\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "from hydra import compose, initialize_config_dir\n",
    "\n",
    "# Retrieve run config from wandb since the run finished before the config was saved\n",
    "api = wandb.Api()\n",
    "run = api.run(\"turing-core/autocast/runs/j7x1q8xq\")\n",
    "\n",
    "# Get resolved config as a dict\n",
    "resolved_config = dict(run.config)\n",
    "\n",
    "# Convert to OmegaConf\n",
    "cfg = OmegaConf.create(resolved_config)[\"hydra\"]\n",
    "\n",
    "cfg.data.data_path = (\n",
    "    \"../datasets/rayleigh_benard/1e3z5x2c_rayleigh_benard_dcae_f32c64_large/\"\n",
    "    \"cache/rayleigh_benard\"\n",
    ")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datamodule and configure processor dimensions\n",
    "\n",
    "from autocast.train.processor import (\n",
    "    configure_processor_dimensions,\n",
    "    prepare_encoded_datamodule,\n",
    ")\n",
    "\n",
    "(\n",
    "    datamodule,\n",
    "    in_channel_count,\n",
    "    out_channel_count,\n",
    "    global_cond_channels,\n",
    "    inferred_n_steps_input,\n",
    "    inferred_n_steps_output,\n",
    "    input_shape,\n",
    "    output_shape,\n",
    "    _example_batch,\n",
    ") = prepare_encoded_datamodule(cfg)\n",
    "\n",
    "configure_processor_dimensions(\n",
    "    cfg,\n",
    "    in_channel_count=in_channel_count,\n",
    "    out_channel_count=out_channel_count,\n",
    "    global_cond_channels=global_cond_channels,\n",
    "    n_steps_input=inferred_n_steps_input,\n",
    "    n_steps_output=inferred_n_steps_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate latent datamodule and setup\n",
    "\n",
    "datamodule = instantiate(cfg.data)\n",
    "datamodule.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Processor\n",
    "processor = instantiate(cfg.model.processor)\n",
    "\n",
    "# Construct ProcessorModelWrapper\n",
    "model = ProcessorModel(processor=processor, learning_rate=cfg.model.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the run directory\n",
    "run_path = \"../outputs/processor/20260117_040704/autocast/j7x1q8xq/\"\n",
    "# config_path = os.path.join(run_path, \"resolved_processor_config.yaml\")\n",
    "ckpt_path = os.path.join(run_path, \"checkpoints/step-step=60000.ckpt\")\n",
    "# ckpt_path = os.path.join(run_path, \"processor.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load(ckpt_path, weights_only=True, map_location=device)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AutoEncoder to decode predictions\n",
    "\n",
    "ae_path = \"../datasets/rayleigh_benard/1e3z5x2c_rayleigh_benard_dcae_f32c64_large\"\n",
    "ae_config_path = os.path.join(ae_path, \"config.yaml\")\n",
    "ae_ckpt_path = os.path.join(ae_path, \"state.pth\")\n",
    "\n",
    "print(f\"Loading AutoEncoder from: {ae_path}\")\n",
    "ae_cfg = OmegaConf.load(ae_config_path)\n",
    "\n",
    "# Convert to dict to avoid OmegaConf/beartype conflicts for args (e.g. attention_heads)\n",
    "ae_config_dict = OmegaConf.to_container(ae_cfg.ae, resolve=True)\n",
    "\n",
    "# However, get_autoencoder specifically types 'loss' as DictConfig, so preserve that\n",
    "if \"loss\" in ae_cfg.ae:\n",
    "    ae_config_dict[\"loss\"] = ae_cfg.ae.loss  # type: ignore  # noqa: PGH003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset mean and std for normalization\n",
    "\n",
    "mean = torch.tensor(ae_cfg.dataset.stats.mean)\n",
    "std = torch.tensor(ae_cfg.dataset.stats.std)\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Encoder and Decoder and load weights from ae_path\n",
    "\n",
    "from autocast.external.lola.wrapped_decoder import WrappedDecoder\n",
    "from autocast.external.lola.wrapped_encoder import WrappedEncoder\n",
    "from autocast.models.autoencoder import AE\n",
    "\n",
    "encoder = WrappedEncoder(\n",
    "    device=device,\n",
    "    runpath=ae_path,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    **ae_config_dict, # type: ignore  # noqa: PGH003\n",
    ")\n",
    "decoder = WrappedDecoder(\n",
    "    device=device,\n",
    "    runpath=ae_path,\n",
    "    mean=mean,\n",
    "    std=std,\n",
    "    **ae_config_dict, # type: ignore  # noqa: PGH003\n",
    ")\n",
    "ae = AE(encoder=encoder, decoder=decoder)\n",
    "_ = ae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambient dataloader\n",
    "\n",
    "with initialize_config_dir(\n",
    "    version_base=None, config_dir=str(Path.cwd() / \"../configs/\")\n",
    "):\n",
    "    data_cfg = compose(\n",
    "        config_name=\"data/the_well\",\n",
    "        overrides=[\"data.well_dataset_name=rayleigh_benard\"],\n",
    "    )[\"data\"]\n",
    "    data_cfg.batch_size = 2\n",
    "    # TODO: for the moment handle the normalization at encoder/decoder level\n",
    "    # data_cfg.use_normalization = True\n",
    "\n",
    "ambient_datamodule = instantiate(data_cfg)\n",
    "ambient_datamodule.setup(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct EncoderProcessorDecoder\n",
    "\n",
    "from autocast.models.encoder_processor_decoder import EncoderProcessorDecoder\n",
    "\n",
    "epd = EncoderProcessorDecoder(encoder_decoder=ae, processor=model.processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch\n",
    "\n",
    "batch = ambient_datamodule.rollout_test_dataloader().__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check autoencoder reconstruction\n",
    "\n",
    "output_recon = ae(batch)\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "im0 = axs[0].imshow(batch.input_fields[0, 0, :, :, 0].detach().cpu().numpy())\n",
    "axs[0].set_title(\"input\")\n",
    "fig.colorbar(im0, ax=axs[0])\n",
    "im1 = axs[1].imshow(output_recon[0, 0, :, :, 0].detach().cpu().numpy())\n",
    "axs[1].set_title(\"recon\")\n",
    "fig.colorbar(im1, ax=axs[1])\n",
    "diff = torch.abs(batch.input_fields - output_recon)\n",
    "im2 = axs[2].imshow(diff[0, 0, :, :, 0].detach().cpu().numpy())\n",
    "axs[2].set_title(\"diff\")\n",
    "fig.colorbar(im2, ax=axs[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rollout on a the batch of trajectories\n",
    "\n",
    "rollout_stride = 4\n",
    "preds, trues = epd.rollout(\n",
    "    batch,\n",
    "    stride=rollout_stride,\n",
    "    max_rollout_steps=25,\n",
    "    free_running_only=True,\n",
    ")\n",
    "print(preds.shape)\n",
    "assert trues is not None\n",
    "print(trues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics computed\n",
    "\n",
    "from autocast.metrics import MSE\n",
    "\n",
    "assert trues is not None\n",
    "assert preds.shape == trues.shape\n",
    "mse = MSE()\n",
    "mse_error = mse(preds, trues).detach().cpu().item()\n",
    "print(f\"MSE overall as a single scalar: {mse_error:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct spatiotemporal video\n",
    "\n",
    "batch_idx = 0\n",
    "metadata = ambient_datamodule.train_dataset.well_metadata\n",
    "simulation_name = metadata.dataset_name\n",
    "anim = plot_spatiotemporal_video(\n",
    "    pred=preds,\n",
    "    true=trues,\n",
    "    batch_idx=batch_idx,\n",
    "    save_path=f\"{simulation_name}_{batch_idx:02d}.mp4\",\n",
    "    colorbar_mode=\"column\",\n",
    "    channel_names=[v for value in metadata.field_names.values() for v in value],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot spatiotemporal video\n",
    "\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
