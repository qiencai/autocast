{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## AutoCast encoder-processor-decoder model API Exploration\n",
    "\n",
    "This notebook aims to explore the end-to-end API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Example dataaset\n",
    "\n",
    "We use the `AdvectionDiffusion` dataset as an example dataset to illustrate training and evaluation of models. This dataset simulates the advection-diffusion equation in 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.metrics.spatiotemporal import MAE, MSE, RMSE\n",
    "\n",
    "THE_WELL = False\n",
    "n_steps_input = 4\n",
    "n_steps_output = 1\n",
    "stride = 1\n",
    "rollout_stride = n_steps_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_steps_input = 4\n",
    "#n_steps_output = 1\n",
    "#original TheWell paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Read combined data into datamodule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached simulation data from reaction_diffusion_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "from autocast.data.utils import get_datamodule\n",
    "\n",
    "if not THE_WELL:\n",
    "    simulation_name = \"reaction_diffusion\"\n",
    "    # simulation_name = \"advection_diffusion\"\n",
    "    # simulation_name = \"advection_diffusion_multichannel\"\n",
    "\n",
    "    if simulation_name == \"advection_diffusion_multichannel\":\n",
    "        # Override to use multichannel version\n",
    "        Sim = AdvectionDiffusionMultichannel\n",
    "    if simulation_name == \"reaction_diffusion\":\n",
    "        Sim = ReactionDiffusion\n",
    "    if simulation_name == \"advection_diffusion\":\n",
    "        Sim = AdvectionDiffusion\n",
    "\n",
    "    sim = Sim(return_timeseries=True, log_level=\"error\")\n",
    "\n",
    "    def generate_split(\n",
    "        simulator, n_train: int = 200, n_valid: int = 20, n_test: int = 20\n",
    "    ):\n",
    "        \"\"\"Generate training, validation, and test splits from the simulator.\"\"\"\n",
    "        train = simulator.forward_samples_spatiotemporal(n_train)\n",
    "        valid = simulator.forward_samples_spatiotemporal(n_valid)\n",
    "        test = simulator.forward_samples_spatiotemporal(n_test)\n",
    "        return {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "\n",
    "    # Cache file path\n",
    "    cache_file = Path(f\"{simulation_name}_cache.pkl\")\n",
    "\n",
    "    # Load from cache if it exists, otherwise generate and save\n",
    "    if cache_file.exists():\n",
    "        print(f\"Loading cached simulation data from {cache_file}\")\n",
    "        with open(cache_file, \"rb\") as f:\n",
    "            combined_data = pickle.load(f)\n",
    "            for key in [\"data\", \"constant_scalars\", \"constant_fields\"]:\n",
    "                combined_data[\"test\"][key] = (\n",
    "                    combined_data[\"test\"][key][:5]\n",
    "                    if combined_data[\"test\"][key] is not None\n",
    "                    else None\n",
    "                )\n",
    "    else:\n",
    "        print(\"Generating simulation data...\")\n",
    "        combined_data = generate_split(sim)\n",
    "        print(f\"Saving simulation data to {cache_file}\")\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(combined_data, f)\n",
    "\n",
    "    datamodule = SpatioTemporalDataModule(\n",
    "        data=combined_data,\n",
    "        data_path = None,\n",
    "        #data_path=\"../datasets/reaction_diffusion\",\n",
    "        n_steps_input=n_steps_input,\n",
    "        n_steps_output=n_steps_output,\n",
    "        stride=n_steps_output,\n",
    "        batch_size=16,\n",
    "    )\n",
    "else:\n",
    "    simulation_name = \"turbulent_radiative_layer_2D\"\n",
    "    datamodule = TheWellDataModule(\n",
    "        well_base_path=\"../../autocast/datasets/\",\n",
    "        well_dataset_name=simulation_name,\n",
    "        n_steps_input=n_steps_input,\n",
    "        n_steps_output=n_steps_output,\n",
    "        min_dt_stride=1,\n",
    "        use_normalization=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 100, 32, 32, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data[\"train\"][\"data\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Set-up logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.logging import maybe_watch_model\n",
    "from autocast.logging.wandb import create_notebook_logger\n",
    "\n",
    "logger, watch = create_notebook_logger(\n",
    "    project=\"autocast-notebooks\",\n",
    "    name=f\"00_01_exploration_{simulation_name}\",\n",
    "    tags=[\"notebook\", simulation_name],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Example shape and batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 32, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datamodule.train_dataset[0].input_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 32, 32, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "\n",
    "batch.input_fields.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f81bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_constant_scalars = batch.constant_scalars.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azula.noise import VPSchedule\n",
    "\n",
    "from autocast.decoders.identity import IdentityDecoder\n",
    "from autocast.encoders.identity import IdentityEncoder\n",
    "from autocast.models.encoder_decoder import EncoderDecoder\n",
    "from autocast.models.encoder_processor_decoder import EncoderProcessorDecoder\n",
    "from autocast.nn.unet import TemporalUNetBackbone\n",
    "from autocast.processors.flow_matching import FlowMatchingProcessor\n",
    "from autocast.processors.fno import FNOProcessor\n",
    "from autocast.processors.vit import AViTProcessor\n",
    "from autocast.encoders.permute_concat import PermuteConcat\n",
    "from autocast.decoders.channels_last import ChannelsLast\n",
    "\n",
    "batch = next(iter(datamodule.train_dataloader()))\n",
    "n_channels = batch.input_fields.shape[-1]\n",
    "\n",
    "processor_name = \"flow_matching\"  # set to \"diffusion\" to compare\n",
    "# processor_name = \"diffusion\"  # set to \"flow_matching\" to compare\n",
    "\n",
    "backbone = TemporalUNetBackbone(\n",
    "    in_channels=n_channels * n_steps_output,\n",
    "    out_channels=n_channels * n_steps_output,\n",
    "    cond_channels=n_channels * n_steps_input,\n",
    "    mod_features=200,\n",
    "    hid_channels=(32, 64, 128),\n",
    "    hid_blocks=(2, 2, 2),\n",
    "    spatial=2,\n",
    "    periodic=False,\n",
    ")\n",
    "\n",
    "if processor_name == \"flow_matching\":\n",
    "    processor = FlowMatchingProcessor(\n",
    "        backbone=backbone,\n",
    "        schedule=VPSchedule(),  # accepted for API parity, not used internally\n",
    "        n_steps_output=n_steps_output,\n",
    "        n_channels_out=n_channels,\n",
    "        stride=stride,\n",
    "        flow_ode_steps=4,\n",
    "    )\n",
    "else:\n",
    "    from autocast.processors.diffusion import DiffusionProcessor\n",
    "\n",
    "    processor = DiffusionProcessor(\n",
    "        backbone=backbone,\n",
    "        schedule=VPSchedule(),\n",
    "        n_steps_output=n_steps_output,\n",
    "        n_channels_out=n_channels,\n",
    "    )\n",
    "\n",
    "encoder = PermuteConcat(with_constants=True)\n",
    "decoder = ChannelsLast(output_channels=n_channels, time_steps=n_steps_output)\n",
    "\n",
    "'''processor = FNOProcessor(\n",
    "    in_channels=(n_channels + n_constant_scalars) * n_steps_input,\n",
    "    out_channels=n_channels * n_steps_output,\n",
    "    n_modes = (16,16)\n",
    ")'''\n",
    "\n",
    "processor = AViTProcessor(\n",
    "    in_channels=(n_channels + n_constant_scalars) * n_steps_input,\n",
    "    out_channels= n_channels * n_steps_output,\n",
    "    spatial_resolution=(32, 32),\n",
    "    hidden_dim=128,\n",
    "    num_heads=8,\n",
    "    n_layers=8,\n",
    "    groups=8\n",
    ")\n",
    "\n",
    "\n",
    "model = EncoderProcessorDecoder(\n",
    "    encoder_decoder=EncoderDecoder(encoder=encoder, decoder=decoder),\n",
    "    processor=processor,\n",
    "    train_processor_only=True,\n",
    "    learning_rate=1e-4,\n",
    "    test_metrics = [MSE(), MAE(), RMSE()],\n",
    "    strie = stride\n",
    ")\n",
    "maybe_watch_model(logger, model, watch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a30ca12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rearrange' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(autocast.encoders.permute_concat.PermuteConcat.encode) at 0x17f67e700>:33\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(__beartype_object_6048233152, __beartype_get_violation, __beartype_conf, __beartype_object_6009286240, __beartype_check_meta, __beartype_func, *args, **kwargs)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/autocast/autocast/src/autocast/encoders/permute_concat.py:32\u001b[39m, in \u001b[36mPermuteConcat.encode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: Batch) -> TensorBNC:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<@beartype(autocast.encoders.permute_concat.PermuteConcat.forward) at 0x17f67e660>:5\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'rearrange' is not defined"
     ]
    }
   ],
   "source": [
    "encoder.encode(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Run trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger.logging.wandb.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "\n",
    "device = \"mps\"  # \"cpu\"\n",
    "# device = \"cpu\"\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=4, accelerator=device, log_every_n_steps=10, logger=logger\n",
    ")\n",
    "trainer.fit(model, datamodule.train_dataloader(), datamodule.val_dataloader())\n",
    "trainer.save_checkpoint(f\"./{simulation_name}_{processor_name}_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Run the evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule.test_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Example rollout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single element is the full trajectory\n",
    "batch = next(iter(datamodule.rollout_test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First n_steps_input are inputs\n",
    "print(batch.input_fields.shape)\n",
    "# Remaining n_steps_output are outputs\n",
    "print(batch.output_fields.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run rollout on one trajectory\n",
    "model.max_rollout_steps = 20\n",
    "preds, trues = model.rollout(batch, stride=rollout_stride, free_running_only=True)\n",
    "\n",
    "print(preds.shape)\n",
    "assert trues is not None\n",
    "print(trues.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocast.metrics.spatiotemporal import MSE\n",
    "\n",
    "assert trues is not None\n",
    "assert preds.shape == trues.shape\n",
    "mse = MSE()\n",
    "mse_error_spatial = mse(preds, trues)\n",
    "mse_error = mse(preds, trues)\n",
    "print(\"MSE spatial has shape (B,T,C):\", mse_error_spatial.shape)\n",
    "print(\"MSE overall is a single scalar:\", mse_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "from autocast.utils import plot_spatiotemporal_video\n",
    "\n",
    "batch_idx = 0\n",
    "if simulation_name == \"advection_diffusion_multichannel\":\n",
    "    channel_names = [\"vorticity\", \"velocity_x\", \"velocity_y\", \"streamfunction\"]\n",
    "elif simulation_name == \"advection_diffusion\":\n",
    "    channel_names = [\"vorticity\"]\n",
    "elif simulation_name == \"reaction_diffusion\":\n",
    "    channel_names = [\"U\", \"V\"]\n",
    "else:\n",
    "    channel_names = None\n",
    "\n",
    "anim = plot_spatiotemporal_video(\n",
    "    pred=preds,\n",
    "    true=trues,\n",
    "    batch_idx=batch_idx,\n",
    "    save_path=f\"{simulation_name}_{batch_idx:02d}.mp4\",\n",
    "    colorbar_mode=\"column\",\n",
    "    channel_names=channel_names,\n",
    ")\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autocast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
