from typing import Any, Self

import lightning as L
import torch
from torch import nn

from auto_cast.decoders import Decoder
from auto_cast.encoders import Encoder
from auto_cast.types import Batch, Tensor, TensorBMStarL, TensorBTSPlusC


class EncoderDecoder(L.LightningModule):
    """Encoder-Decoder Model."""

    encoder: Encoder
    decoder: Decoder
    loss_func: nn.Module | None
    learning_rate: float = 1e-3

    def __init__(self):
        super().__init__()

    @classmethod
    def from_encoder_decoder(
        cls,
        encoder: Encoder,
        decoder: Decoder,
        loss_func: nn.Module | None = None,
        **kwargs: Any,
    ) -> Self:
        instance = cls(**kwargs)
        instance.encoder = encoder
        instance.decoder = decoder
        instance.loss_func = loss_func
        return instance

    def forward(self, batch: Batch) -> TensorBTSPlusC:
        return self.decoder(self.encoder(batch))

    def forward_with_latent(self, batch: Batch) -> tuple[TensorBTSPlusC, TensorBMStarL]:
        encoded = self.encode(batch)
        decoded = self.decode(encoded)
        return decoded, encoded

    def training_step(self, batch: Batch, batch_idx: int) -> Tensor:  # noqa: ARG002
        if self.loss_func is None:
            msg = "Loss function not defined for EncoderDecoder model."
            raise ValueError(msg)
        x = self(batch)
        output = self.decoder(x)
        loss = self.loss_func(output, batch.output_fields)
        self.log(
            "train_loss", loss, prog_bar=True, batch_size=batch.input_fields.shape[0]
        )
        return loss

    def validation_step(self, batch: Batch, batch_idx: int) -> Tensor:  # noqa: ARG002
        output = self(batch)
        if self.loss_func is None:
            msg = "Loss function not defined for EncoderDecoder model."
            raise ValueError(msg)
        loss = self.loss_func(output, batch.output_fields)
        self.log(
            "val_loss", loss, prog_bar=True, batch_size=batch.input_fields.shape[0]
        )
        return loss

    def predict_step(self, batch: Batch, batch_idx: int) -> TensorBTSPlusC:  # noqa: ARG002
        return self(batch)

    def encode(self, batch: Batch) -> TensorBMStarL:
        return self.encoder.encode(batch)

    def decode(self, z: TensorBMStarL) -> TensorBTSPlusC:
        return self.decoder.decode(z)

    def configure_optimizers(self):
        """Configure optimizers for training."""
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)


class VAE(EncoderDecoder):
    """Variational Autoencoder Model."""

    def forward(self, batch: Batch) -> Tensor:
        mu, log_var = self.encoder(batch)
        z = self.reparametrize(mu, log_var)
        x = self.decoder(z)
        return x  # noqa: RET504

    def reparametrize(self, mu: Tensor, log_var: Tensor) -> Tensor:
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std
